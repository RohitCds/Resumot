# ğŸ§  Resumot: Resume Chat via Local LLM (GGUF Inference)

This project lets you **chat with a pile of PDF resumes** using a **local Large Language Model** running in `.gguf` format via `llama-cpp-python`.  
No API keys. No cloud hosting. Pure local inference. ğŸ”’

---

## ğŸš€ Features

- ğŸ“„ Upload one or more PDF resumes
- ğŸ§  Chunk and embed resumes using **ChromaDB + Sentence Transformers**
- ğŸ” Ask natural language questions like:
  - "Who worked at Google in 2020?"
  - "Which company does John currently work at?"
  - "Who has experience with React?"
- ğŸ¤– Get answers from a **quantized DeepSeek model** running locally (no API needed)

---

## ğŸ—ï¸ Project Structure
â”œâ”€â”€ main.py # Streamlit UI
â”œâ”€â”€ query_engine.py # GGUF model inference with context injection
â”œâ”€â”€ embedder.py # ChromaDB + SentenceTransformer-based embedding + retrieval
â”œâ”€â”€ resume_parser.py # PDF text extraction and basic chunking
â”œâ”€â”€ sample_resumes/ # Folder for uploaded resumes
â”œâ”€â”€ models/ # Place your GGUF model file here
â””â”€â”€ requirements.txt # Dependencies


---

## ğŸ“¦ Requirements

Create a virtual environment and install dependencies:

```bash
pip install -r requirements.txt
```
---

ğŸ¤– Model Info
This project uses a quantized DeepSeek Coder 13B (Instruct) model in .gguf format.

ğŸ“¥ Download the model from Hugging Face:

ğŸ‘‰ https://huggingface.co/Theros/Q2.5-DeepSeek-R1-DeepThink-test1-Q4_K_M-GGUF

Recommended file: Theros/Q2.5-DeepSeek-R1-DeepThink-test1-Q4_K_M-GGUF

Place it in:
```
~/Desktop/Models/
```
---

ğŸ§ª Running the App
```
streamlit run main.py
```
Open your browser to http://localhost:8501 and start chatting with your resumes!

---

ğŸ’¡ How It Works
You upload PDF resumes.

resume_parser.py extracts the raw text.

embedder.py chunks the text and embeds it into a ChromaDB collection using all-mpnet-base-v2.

When you ask a question:

query_engine.py retrieves the most relevant chunks using vector similarity

The question + context is sent to the DeepSeek 13B Instruct model for response generation

The result is streamed back in the UI.

---

ğŸ“ Notes
This is the inference-only edition â€” no fine-tuning or tagging is involved.

If you're on macOS without a GPU, the model still runs thanks to CPU inference using llama.cpp.

Consider reducing context size or switching to a smaller model if your system is resource-constrained.

---

ğŸ“£ Credits
DeepSeek for the original DeepSeek-Coder model

Theros for the quantized .gguf version: Q2.5-DeepSeek-R1-DeepThink-Q4_K_M

ChromaDB for efficient local vector database

llama-cpp-python for fast local LLM inference in GGUF format

---

ğŸ§  Future Additions
Resume skill/tag extraction via fine-tuning (currently commented in code)

Multi-user interface with persistent resume sessions

CSV export for filtered candidate insights

---

ğŸ”“ License
MIT â€” free to use, fork, and modify. Just give credit if you find this useful!



